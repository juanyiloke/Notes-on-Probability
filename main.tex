%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
%
% This is where the packages are declared

\documentclass[]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}

\begin{document}

%
% = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
%
% The Title, Author Name, Date, and abstract go here

\title{Notes on Probability, Reinforcement Learning, and Mathematical Concepts Encountered Through the ROP at Williams' Lab}
\author{Juan Yi Loke}
\date{\today}

\maketitle


\noindent \textbf{Author's Note:} These notes might contain errors or are incomplete. Use them at your own risk!


%
% = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%











%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
% Here is where your content goes.
%

%------------------------------------------------------------------------------------------------------------
%------------------------------------------------------------------------------------------------------------
\section{Decision Theory}
%------------------------------------------------------------------------------------------------------------
%------------------------------------------------------------------------------------------------------------

\subsection{What is Decision Theory?}
To solve the problem of inference, we have four steps:

\begin{enumerate}
    \item Enumerate the possible states of nature $\theta_j$, discrete or continuous, as the case may be.
    \item Assign prior probabilities $p(\theta_j\mid I)$ which represents whatever prior information $I$ we have about them. 
    \item Assign sampling probabilities $p(E_i \mid \theta_j)$ which represent our prior knowledge about the mechanism of the measurement process yielding the possible data sets $E_i$.
    \item Digest any additional evidence $E = E_1E_2.....$ by application of Bayes' Theorem, thus obtaining the posterior probabilities $p(\theta_j \mid EI)$. 
\end{enumerate}

\noindent That is all we need to solve the inference problem. Now to solve the problem of decision, we add 3 more steps onto the problem of inference:

\begin{enumerate}
    \setcounter{enumi}{4}
    \item Enumerate the possible decisions $D_i$.
    \item Assign the loss function $L(D_i, \theta_j)$ that tells us what we want to accomplish.
    \item Make that decision $D_i$, which minimizes the expected loss over the posterior probabilities for $\theta_j$.
\end{enumerate}

\noindent Note that a loss function is a function that maps an event or values of one or more variables onto a real number which represents some "cost" associated with the event. Any optimization problem seeks to minimize a loss function. In other domains, it can be called a reward function, profit function, utility function, fitness function, etv), in which case it is to be maximized. Maximization and minimization is fundamentally the same operation. 

\section{Information Theory}
This section is notes from the text 'Probability and Information' by David Applebaum.
\subsection{A couple of things to note beforehand}
I found this sub-chapter particularly insightful as it gives a nice overview of what math is all about.\\

\noindent A mathematical theory is a systematic exposition of all the knolwedge we possess about a certain area. The essence of a mathematical theory is to begin with some basic definitions, called axioms, which describe the main mathematical objects we are investigating.\\

\noindent We then use clear and precise logical arguments to deduce the properties of these objects. These new properties are usually announced in statements called theorems and the arguments that we use to convince ourselves of the validity of these theorems are proofs.\\

\noindent As the theory develops and we learn new concepts that are needed in addition to those given in the axioms, we introduce these new concepts as definitions.\\

\noindent Some mathematical nomenclature for statements:
\begin{enumerate}
    \item Lemma - a minor technical result which serves as a stepping stone towards a theorem.
    \item Proposition - in between a lemma and a theorem. Sometimes it indicates a theorem from a different branch of mathematics is needed so that it can be applied within the current theory. 
    \item Corollary - A result which follows almost immediately from the theorem with very little argument. 
\end{enumerate}

\subsection{Information Theory}


\section{Probability}
Probability is a mathematical term used to investigate properties of mathematical models of chance phenomena, also called probabilistic models. 'Probability' does not exist out in the real world. However, the applications of the subject spread into every corner of science and modern life. It is the foundation of the science of statistics as well as of statistical mechanics. This section will give mathematical meaning to the concept of 'information' from the framework of probability. 


\section{Reinforcement Learning}
\subsection{What I gathered from the IAI lab}
Beforehand, we will consider what is called the Markov Decision Process (MDP). An MDP is an extension of Markov Chains


\end{document}